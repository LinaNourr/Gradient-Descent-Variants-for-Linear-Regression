# Gradient-Descent-Variants-for-Linear-Regression
This repository contains an in-depth exploration of various gradient descent algorithms applied to linear regression. The primary objective is to compare and contrast different gradient descent methods to understand their efficiency, convergence rates, and suitability for linear regression problems.

Introduction
Gradient descent is a fundamental optimization technique widely used in machine learning and statistical modeling. This project focuses on implementing and analyzing different variants of gradient descent to solve linear regression problems. The notebook includes detailed explanations and visualizations to aid in understanding the mechanics and performance of each algorithm.

Algorithms Covered
Batch Gradient Descent (BGD): Uses the entire dataset to compute gradients.
Stochastic Gradient Descent (SGD): Uses a single sample to compute gradients, leading to faster but noisier updates.
Mini-Batch Gradient Descent (MBGD): Combines aspects of both BGD and SGD by using small batches of data for each update.
